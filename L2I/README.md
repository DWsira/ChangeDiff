# Layout to Image Diffusion

## Start
Please clone the repository and set up the environment:
```
git clone https://github.com/DZhaoXd/ChangeDiff.git
cd L2I
conda env create -f environment.yaml
conda activate changediff
```

Please download the pre-trained Stable Diffusion model
```
mkdir models/ldm/stable-diffusion
wget -O models/ldm/stable-diffusion/sd-v1-4-full-ema.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt
```

Dataset preparation : Store the dataset path in json file.

**SECOND** dataset can be downloaded [here](https://captain-whu.github.io/SCD/) and put it under [](./data)

Example of json files is in [here](./data/SECOND_train_0.8.json).

## Training

To train L2I, run the script [](./train_SECOND.sh)
```shell
CUDA_VISIBLE_DEVICES=0 python main.py --base ./configs/stable-diffusion/v1-finetune_SECOND.yaml \
    -t \
    --actual_resume ./models/ldm/stable-diffusion/sd-v1-4-full-ema.ckpt \
    -n exp_SECOND_from_SCD \
    --gpus 0, \
    --data_root ./data/SECOND \
    --no-test \
    --json_file ./data/SECOND_train_0.8.json
```

## Generation

Before generating images, [a json file](./data/sample_4.json) containing layout path information generated by T2I model is needed.

To generate images using L2I, run the script [](./sample_SECOND_AB.sh)
```shell
CUDA_VISIBLE_DEVICES=0 python LIS_AB.py --batch_size 8 \
    --config ./configs/stable-diffusion/v1-finetune_SECOND.yaml \
    --ckpt ./logs/exp_SECOND_from_SCD/checkpoints/last.ckpt \
    --dataset SECOND \
    --outdir ./outputs/SECOND_LIS_AB \
    --txt_file ./data/sample_4.json \
    --data_root ./data/SECOND \
    --plms
```

We provide the trained SECOND dataset weights [here](https://drive.google.com/file/d/1m0j4ej6ELfDyHSOAupRX8NYKkMJvvURM/view?usp=drive_link).

## Acknowledgments
Our code borrows heavily from [FreestyleNet](https://github.com/essunny310/FreestyleNet)